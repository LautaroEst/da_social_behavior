{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    COLUMBIA_PATH = \"../data/ColumbiaGamesCorpus/\"\n",
    "    social_variables = pd.read_csv(\n",
    "        os.path.join(COLUMBIA_PATH,\"mturk/MTurkData.csv\")\n",
    "    )\n",
    "    words = {}\n",
    "    turns = {}\n",
    "    for n_sess in range(1,13):\n",
    "        words[n_sess] = {}\n",
    "        turns[n_sess] = {}\n",
    "        df_tasks = pd.read_csv(\n",
    "            os.path.join(COLUMBIA_PATH,f\"data/session_{n_sess:02}/s{n_sess:02}.objects.1.tasks\"),\n",
    "            sep=\" \",\n",
    "            header=None,\n",
    "            names=[\"start_time\",\"end_time\",\"labels\"]\n",
    "        )\n",
    "        df_tasks = df_tasks[df_tasks[\"labels\"].str.contains(\"Images\")].reset_index(drop=True)\n",
    "        df_tasks.index += 1\n",
    "        for n_task in range(1,15):\n",
    "            words[n_sess][n_task] = {}\n",
    "            turns[n_sess][n_task] = {}\n",
    "            start_time = df_tasks.loc[n_task,\"start_time\"]\n",
    "            end_time = df_tasks.loc[n_task,\"end_time\"]\n",
    "            for speaker in [\"A\",\"B\"]:\n",
    "                w = pd.read_csv(\n",
    "                    os.path.join(COLUMBIA_PATH,f\"data/session_{n_sess:02}/s{n_sess:02}.objects.1.{speaker}.words\"),\n",
    "                    sep=\" \",\n",
    "                    header=None,\n",
    "                    names=[\"start_time\",\"end_time\",f\"word_{speaker}\"]\n",
    "                )\n",
    "                w = w[(w[\"start_time\"] >= start_time) & (w[\"end_time\"] <= end_time)].reset_index(drop=True)\n",
    "                words[n_sess][n_task][speaker] = w.loc[:,[\"start_time\",\"end_time\",f\"word_{speaker}\"]]\n",
    "                \n",
    "                t = pd.read_csv(\n",
    "                    os.path.join(COLUMBIA_PATH,f\"data/session_{n_sess:02}/s{n_sess:02}.objects.1.{speaker}.turns\"),\n",
    "                    sep=\" \",\n",
    "                    header=None,\n",
    "                    names=[\"start_time\",\"end_time\",f\"turn_{speaker}\"]\n",
    "                )\n",
    "                t = t[(t[\"start_time\"] >= start_time) & (t[\"end_time\"] <= end_time)].reset_index(drop=True)\n",
    "                turns[n_sess][n_task][speaker] = t.loc[:,[\"start_time\",\"end_time\",f\"turn_{speaker}\"]]\n",
    "                \n",
    "    social_variables = social_variables.set_index(\"hitid\")\n",
    "    return words, turns, social_variables\n",
    "\n",
    "\n",
    "def print_aligned(df,width=80):\n",
    "    n_batches = int(np.ceil(len(df) / width))\n",
    "    \n",
    "    def align_words(words):\n",
    "        lens = [len(w) for w in words]\n",
    "        max_len_word = np.argmax(lens)\n",
    "        aligned_words = [w + \" \" * (len(words[max_len_word])-len(w)) for w in words]\n",
    "        return aligned_words\n",
    "    \n",
    "    aligned_columns = align_words(df.columns)\n",
    "    for i in range(n_batches):\n",
    "        aligned_rows = [(column, []) for column in aligned_columns]\n",
    "        for j in range(i*width,(i+1)*width):\n",
    "            if j >= len(df):\n",
    "                break\n",
    "            aligned_words = align_words(df.loc[j,:].values)\n",
    "            for r, w in zip(aligned_rows,aligned_words):\n",
    "                r[1].append(w)\n",
    "        for c, row in aligned_rows:\n",
    "            print(f\"{c}: {' '.join(row)}\")\n",
    "        print()\n",
    "\n",
    "def merge_turns(words):\n",
    "    dfs = {}\n",
    "    for n_sess in range(1,13):\n",
    "        dfs[n_sess] = {}\n",
    "        for n_task in range(1,15):\n",
    "            df_A = words[n_sess][n_task][\"A\"].rename(columns={\"word_A\": \"word\"})\n",
    "            df_A[\"speaker\"] = \"A\"\n",
    "            df_B = words[n_sess][n_task][\"B\"].rename(columns={\"word_B\": \"word\"})\n",
    "            df_B[\"speaker\"] = \"B\"\n",
    "            df = pd.concat([df_A,df_B],ignore_index=True,axis=0)\n",
    "            df = df.sort_values(by=[\"start_time\",\"end_time\"],ascending=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            utts = []\n",
    "            utt = []\n",
    "            is_talking = \"None\"\n",
    "            for i, row in df.iterrows():\n",
    "                if row[\"word\"] != \"#\" and is_talking == \"None\":\n",
    "                    utt.append(row[\"word\"])\n",
    "                    is_talking = row[\"speaker\"]\n",
    "                elif row[\"word\"] != \"#\" and is_talking != \"None\":\n",
    "                    if is_talking == row[\"speaker\"]:\n",
    "                        utt.append(row[\"word\"])\n",
    "                    else:\n",
    "                        utts.append({\"utt\": utt, \"speaker\": is_talking})\n",
    "                        utt = [row[\"word\"]]\n",
    "                    is_talking = row[\"speaker\"]\n",
    "                elif len(utt) > 0:\n",
    "                    utts.append({\"utt\": utt, \"speaker\": is_talking})\n",
    "                    utt = []\n",
    "                    is_talking = \"None\"\n",
    "                else:\n",
    "                    pass\n",
    "            dfs[n_sess][n_task] = {}\n",
    "            dfs[n_sess][n_task][\"df\"] = pd.DataFrame.from_records(utts)\n",
    "            dfs[n_sess][n_task][\"df_orig\"] = df\n",
    "            dfs[n_sess][n_task][\"sv\"] = social_variables.loc[f\"g{n_sess}t{n_task}\",:]\n",
    "    return dfs\n",
    "    \n",
    "words, turns, social_variables = load_data()\n",
    "dfs = merge_turns(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['okay'] A\n",
      "['so', 'the', 'nail'] A\n",
      "['is', 'gonna', 'go', 'directly', 'on', 'top', 'of', 'the', 'lawnmower'] A\n",
      "['and'] A\n",
      "['if', 'you', 'look', 'at', 'the', 'nail', \"there's\"] A\n",
      "['a', 'line'] A\n",
      "['n-', 'kind', 'of', 'see', 'the', \"nail's\", 'pointed', 'and', 'then', \"there's\", 'a', 'line', 'that', 'kinda', 'cuts', 'across'] A\n",
      "['mmhm'] B\n",
      "['um'] A\n",
      "['mmhm'] B\n",
      "['that', 'looks', 'like', 'it', 'gets', 'lined', 'up', 'with', 'the', 'front', 'edge', 'of', 'the', 'lawnmower'] A\n",
      "['oh', 'okay', 'I', 'see', 'what', \"you're\", 'saying'] B\n",
      "['and'] A\n",
      "['so'] B\n",
      "[\"it's\", 'probably', 'like', 'half', 'an', 'inch', 'maybe'] B\n",
      "['no', 'no', 'no'] B\n",
      "['mm'] A\n",
      "['that', 'that', 'line', 'tha-'] B\n",
      "['mmhm'] A\n",
      "['near', 'the', 'point', 'of', 'the', 'nail', 'that', 'lines', 'up', 'with', 'the', 'front', 'edge', 'of', 'the', 'lawnmower'] B\n",
      "['yeah'] A\n",
      "['like', 'r-'] A\n",
      "['like', 'right', 'on', 'top', 'of', 'it'] A\n",
      "['maybe', 'slightly', 'in', 'front', 'of', 'it'] A\n",
      "['just', 'slight', 'slight', 'slight'] A\n",
      "['and', 'then', 'the', 'head', 'of', 'the', 'nail', 'kind', 'of', 'falls', 'between', 'the'] A\n",
      "['um', 'handlebars', 'the'] A\n",
      "['the', 'two', 'like', 'parallel', 'bars'] A\n",
      "['on', 'the'] A\n",
      "['pushing', 'part', 'of', 'the', 'lawnmower'] A\n",
      "['um'] B\n",
      "['so', 'how', 'far', 'does', 'the', 'nail', 'stick', 'out'] B\n",
      "['in', 'front', 'of', 'the'] B\n",
      "['um'] A\n",
      "['the'] A\n",
      "['lawnmower'] B\n",
      "['it'] A\n",
      "['looks', 'like', 'that', 'line'] A\n",
      "['pretty', 'much', 'lines', 'up', 'with', 'the', 'edge', 'so', 'the', 'tip', 'kind', 'of', 'comes', 'over'] A\n",
      "['so', 'the', 'tip', 'the', 'very', 'point', 'of', 'it'] A\n",
      "['is'] A\n",
      "['um'] A\n",
      "['I', 'know', 'what', \"you're\", 'asking'] A\n",
      "['like', 'the', 'the', 'black', 'line', 'pretty', 'much', 'lines', 'up', 'with', 'the', 'front', 'of', 'it', 'so'] A\n",
      "['right'] B\n",
      "['the', 'tip'] A\n",
      "['the', 'tip', 'is', 'over'] A\n",
      "['right'] B\n",
      "['and', 'then', 'the', 'head', 'of', 'the', 'nail', 'is', 'y-', 'covers', 'the', 'handlebar', 'so', 'you', \"can't\", 'really', 'see', 'it'] A\n",
      "['the', 'the', 'top', 'part', 'of', 'the', 'handlebar'] A\n",
      "['and', \"they're\", 'very', 'close', 'together'] B\n",
      "['um', 'what', 'do', 'you', 'mean'] A\n",
      "['th-', 'the', 'nail', 'and', 'the', 'lawnmower', \"they're\", 'pretty', 'much', 'on', 'top', 'of', 'each', 'other'] B\n",
      "['yeah', 'they', 'are', 'on', 'top', 'of', 'each', 'other'] A\n",
      "['like', 'the', 'nail', 'is', 'directly', 'on', 'top', 'of', 'it'] A\n",
      "['no', 'pixelated', 'space'] B\n",
      "['um'] A\n",
      "['no', 'w-', 'um'] A\n",
      "['what', 'do', 'you', 'mean'] A\n",
      "['between', 'the', 'n-', 'head', 'of', 'the', 'nail', 'and', 'the', 'handlebar', 'is'] B\n",
      "['no'] A\n",
      "['there', 'any', 'space'] B\n",
      "['you'] A\n",
      "[\"can't\", 'the', 'head', 'of', 'the', 'nail'] A\n",
      "['the'] A\n",
      "['yeah'] B\n",
      "['the', 'flat'] A\n",
      "['part', 'of', 'the', 'nail'] A\n",
      "['right'] B\n",
      "['is', 'covering'] A\n",
      "['the', 'top', 'part', 'of', 'the', 'handlebar', 'where', 'you', 'would', 'grip', 'the', 'lawnmower', 'you'] A\n",
      "['okay'] B\n",
      "[\"can't\", 'see'] A\n",
      "['that', 'at', 'all'] A\n",
      "['at', 'all'] B\n",
      "['at', 'all'] A\n",
      "['so', 'I', 'would', 'say', 'if', 'you', 'look', 'at', 'the', 'head', 'of', 'the', 'nail'] A\n",
      "['um'] A\n",
      "['and', 'kinda', 'drew', 'a', 'straight', 'line', 'through', 'it'] A\n",
      "['like'] A\n",
      "['it', 'would', 'cut', 'like', 'it', 'would', 'it', 'would', 'line', 'up', 'with', 'the', 'handlebar'] A\n",
      "['okay'] B\n",
      "['wait'] B\n",
      "['wait'] B\n",
      "['is', 'the', 'nail', 'like', 'literally', 'on', 'top', 'of', 'the', 'lawnmower'] B\n",
      "['literally'] A\n",
      "['okay'] B\n",
      "['okay', 'okay'] B\n",
      "['yeah'] A\n",
      "['I'] B\n",
      "['was'] B\n",
      "['doing', 'something', 'completely', 'different'] B\n",
      "['oh'] A\n",
      "['yeah', \"it's\", 'like', 'on', 'top', 'of', 'it'] A\n",
      "['okay', 'so'] B\n",
      "['the'] B\n",
      "['below', 'the', 'head', 'of', 'the', 'nail'] B\n",
      "[\"there's\", 'like', 'a', 'ring', 'like', 'a', 'rung'] B\n",
      "['a', 'bottom', 'line'] B\n",
      "['right'] A\n",
      "['I', 'see', 'that', 'that', 'lines', 'up'] A\n",
      "['with'] B\n",
      "['w-'] A\n",
      "['the', 'middle', 'crossbar'] B\n",
      "['correct'] A\n",
      "['okay'] B\n",
      "['yeah'] A\n"
     ]
    }
   ],
   "source": [
    "for _, row in dfs[2][4][\"df\"].iterrows():\n",
    "    print(row[\"utt\"],row[\"speaker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word   : okay so uh the lawnmower # is right in between of # the two lions and the nail # so\n",
      "speaker: A    A  A  A   A         A A  A     A  A       A  A A   A   A     A   A   A    A A \n",
      "\n",
      "word   : right in the # the middle of them there # to the left of the nail mmhm\n",
      "speaker: A     A  A   A A   A      A  A    A     A A  A   A    A  A   A    B   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_aligned(dfs[1][2][\"df_orig\"].loc[:,[\"word\",\"speaker\"]],width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_sess in range(1,13):\n",
    "    for n_task in range(1,15):\n",
    "        s = sum(dfs[n_sess][n_task][\"df\"][\"utt\"].apply(lambda x: len(x) == 0))\n",
    "        if s > 0:\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_input(dfs,n_sess,n_task):\n",
    "    speaker = dfs[n_sess][n_task][\"df\"].loc[0,\"speaker\"]\n",
    "    turns = []\n",
    "    turn = []\n",
    "    for i, utt in dfs[n_sess][n_task][\"df\"].iterrows():\n",
    "        if utt.speaker != speaker:\n",
    "            turns.append(\" \".join(turn))\n",
    "            turn = utt.utt\n",
    "        else:\n",
    "            turn.extend(utt.utt)\n",
    "        speaker = utt.speaker\n",
    "    turns = \" <TURN> \".join(turns)\n",
    "    return turns\n",
    "\n",
    "inp = create_input(dfs,n_sess=1,n_task=4)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b7cc45abe94028a5dbf23708712513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/369 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbed411c2fe418db0f544a983993e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4088f774b84af182b53c21e9ee2b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c04c60d6dd248c39e61ff15c9059cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6bd84bf0df4a60a9cec15715570daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d327c12a0147d192d3c1b5db095b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff3bde9a07544fe98f7c826e84886fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94cbce785134e2d8706bbdabe1769dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/593M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pzelasko/longformer-swda-nolower were not used when initializing LongformerForTokenClassification: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"pzelasko/longformer-swda-nolower\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"pzelasko/longformer-swda-nolower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='pzelasko/longformer-swda-nolower', vocab_size=50265, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False), 'additional_special_tokens': ['<TURN>']})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: <s> ok              ay              Ġso Ġthe Ġnail Ġis Ġgonna Ġgo Ġdirectly\n",
      "da   : sd  fo_o_fw_\"_by_bc fo_o_fw_\"_by_bc I-  I-   I-    I-  I-     I-  I-       \n",
      "\n",
      "words: Ġon Ġtop Ġof Ġthe Ġlawn m  ower Ġand Ġif Ġyou\n",
      "da   : I-  I-   I-  I-   sd    sd sd   sd   I-  I-  \n",
      "\n",
      "words: Ġlook Ġat Ġthe Ġnail Ġthere 's Ġa Ġline Ġn - \n",
      "da   : I-    I-  I-   I-    I-     I- I- I-    I- sd\n",
      "\n",
      "words: Ġkind Ġof Ġsee Ġthe Ġnail 's Ġpointed Ġand Ġthen Ġthere\n",
      "da   : I-    I-  I-   I-   I-    I- sd       sd   I-    I-    \n",
      "\n",
      "words: 's Ġa Ġline Ġthat Ġkinda Ġcuts Ġacross Ġ  <TURN> Ġmm\n",
      "da   : I- I- I-    I-    I-     I-    I-      sd O      I- \n",
      "\n",
      "words: hm Ġ <TURN> Ġum Ġ <TURN> Ġmm hm Ġ <TURN>\n",
      "da   : I- b O      %   % O      I-  b  b O     \n",
      "\n",
      "words: Ġthat Ġlooks Ġlike Ġit Ġgets Ġlined Ġup Ġwith Ġthe Ġfront\n",
      "da   : I-    I-     I-    I-  I-    I-     I-  I-    I-   I-    \n",
      "\n",
      "words: Ġedge Ġof Ġthe Ġlawn m  ower Ġ  <TURN> Ġoh Ġokay\n",
      "da   : I-    I-  I-   sd    sd sd   sd O      I-  b    \n",
      "\n",
      "words: ĠI Ġsee Ġwhat Ġyou 're Ġsaying Ġ <TURN> Ġand Ġ\n",
      "da   : I- I-   I-    I-   I-  I-      % O      I-   %\n",
      "\n",
      "words: <TURN> Ġso Ġit 's Ġprobably Ġlike Ġhalf Ġan Ġinch Ġmaybe\n",
      "da   : O      I-  I-  I- I-        I-    I-    I-  I-    sd    \n",
      "\n",
      "words: Ġno Ġno Ġno Ġ <TURN> Ġmm Ġ <TURN> Ġthat Ġthat\n",
      "da   : nn  ar  ar  % O      %   b O      I-    I-   \n",
      "\n",
      "words: Ġline Ġtha - Ġ <TURN> Ġmm hm Ġ <TURN> Ġnear\n",
      "da   : I-    I-   % % O      I-  b  b O      I-   \n",
      "\n",
      "words: Ġthe Ġpoint Ġof Ġthe Ġnail Ġthat Ġlines Ġup Ġwith Ġthe\n",
      "da   : I-   I-     I-  I-   I-    I-    I-     I-  I-    I-  \n",
      "\n",
      "words: Ġfront Ġedge Ġof Ġthe Ġlawn m  ower Ġ  <TURN> Ġyeah\n",
      "da   : I-     I-    I-  I-   I-    I- I-   sd O      b    \n",
      "\n",
      "words: Ġlike Ġr - Ġlike Ġright Ġon Ġtop Ġof Ġit Ġmaybe\n",
      "da   : I-    I- % I-    I-     I-  I-   I-  I-  I-    \n",
      "\n",
      "words: Ġslightly Ġin Ġfront Ġof Ġit Ġjust Ġslight Ġslight Ġslight Ġand\n",
      "da   : I-        I-  I-     I-  sd  I-    I-      I-      sd      I-  \n",
      "\n",
      "words: Ġthen Ġthe Ġhead Ġof Ġthe Ġnail Ġkind Ġof Ġfalls Ġbetween\n",
      "da   : I-    I-   I-    I-  I-   I-    I-    I-  I-     I-      \n",
      "\n",
      "words: Ġthe Ġum Ġhandle bars Ġthe Ġthe Ġtwo Ġlike Ġparallel Ġbars\n",
      "da   : I-   I-  I-      I-   I-   I-   I-   I-    I-        I-   \n",
      "\n",
      "words: Ġon Ġthe Ġpushing Ġpart Ġof Ġthe Ġlawn m  ower Ġ \n",
      "da   : I-  I-   I-       I-    I-  I-   sd    sd sd   sd\n",
      "\n",
      "words: <TURN> Ġum Ġso Ġhow Ġfar Ġdoes Ġthe Ġnail Ġstick Ġout\n",
      "da   : O      I-  I-  I-   I-   I-    I-   I-    I-     I-  \n",
      "\n",
      "words: Ġin Ġfront Ġof Ġthe Ġ  <TURN> Ġum Ġthe Ġ <TURN>\n",
      "da   : I-  I-     I-  I-   qw O      I-  I-   % O     \n",
      "\n",
      "words: Ġlawn m  ower Ġ  <TURN> Ġit Ġlooks Ġlike Ġthat Ġline\n",
      "da   : sd    qw qw   qw O      I-  I-     I-    I-    I-   \n",
      "\n",
      "words: Ġpretty Ġmuch Ġlines Ġup Ġwith Ġthe Ġedge Ġso Ġthe Ġtip\n",
      "da   : I-      I-    I-     I-  I-    I-   sd    I-  I-   I-  \n",
      "\n",
      "words: Ġkind Ġof Ġcomes Ġover Ġso Ġthe Ġtip Ġthe Ġvery Ġpoint\n",
      "da   : I-    I-  I-     sd    I-  I-   I-   I-   I-    I-    \n",
      "\n",
      "words: Ġof Ġit Ġis Ġum ĠI Ġknow Ġwhat Ġyou 're Ġasking\n",
      "da   : I-  I-  I-  I-  sd I-    I-    I-   I-  I-     \n",
      "\n",
      "words: Ġlike Ġthe Ġthe Ġblack Ġline Ġpretty Ġmuch Ġlines Ġup Ġwith\n",
      "da   : I-    I-   I-   I-     I-    I-      I-    I-     I-  I-   \n",
      "\n",
      "words: Ġthe Ġfront Ġof Ġit Ġso Ġ <TURN> Ġright Ġ <TURN>\n",
      "da   : I-   I-     I-  sd  I-  % O      b      b O     \n",
      "\n",
      "words: Ġthe Ġtip Ġthe Ġtip Ġis Ġover Ġ  <TURN> Ġright Ġ\n",
      "da   : I-   I-   I-   I-   I-  sd    sd O      b      b\n",
      "\n",
      "words: <TURN> Ġand Ġthen Ġthe Ġhead Ġof Ġthe Ġnail Ġis Ġy\n",
      "da   : O      I-   I-    I-   I-    I-  I-   I-    I-  I-\n",
      "\n",
      "words: -  Ġcovers Ġthe Ġhandle bar Ġso Ġyou Ġcan 't Ġreally\n",
      "da   : sd I-      I-   sd      sd  I-  I-   I-   I- I-     \n",
      "\n",
      "words: Ġsee Ġit Ġthe Ġthe Ġtop Ġpart Ġof Ġthe Ġhandle bar\n",
      "da   : I-   I-  I-   I-   I-   I-    I-  I-   sd      sd \n",
      "\n",
      "words: Ġ  <TURN> Ġand Ġthey 're Ġvery Ġclose Ġtogether Ġ  <TURN>\n",
      "da   : sd O      I-   I-    I-  I-    I-     sd        sd O     \n",
      "\n",
      "words: Ġum Ġwhat Ġdo Ġyou Ġmean Ġ  <TURN> Ġth - Ġthe\n",
      "da   : I-  I-    I-  I-   qy    qw O      I-  % I-  \n",
      "\n",
      "words: Ġnail Ġand Ġthe Ġlawn m  ower Ġthey 're Ġpretty Ġmuch\n",
      "da   : I-    I-   I-   I-    I- I-   I-    I-  I-      I-   \n",
      "\n",
      "words: Ġon Ġtop Ġof Ġeach Ġother Ġ  <TURN> Ġyeah Ġthey Ġare\n",
      "da   : I-  I-   I-  sd    sd     sd O      ny    I-    I-  \n",
      "\n",
      "words: Ġon Ġtop Ġof Ġeach Ġother Ġlike Ġthe Ġnail Ġis Ġdirectly\n",
      "da   : I-  I-   I-  sd    sd     I-    I-   I-    I-  I-       \n",
      "\n",
      "words: Ġon Ġtop Ġof Ġit Ġ  <TURN> Ġno Ġpixel ated Ġspace\n",
      "da   : I-  I-   I-  sd  sd O      I-  I-     I-   qy^d  \n",
      "\n",
      "words: Ġ <TURN> Ġum Ġno Ġw - Ġum Ġwhat Ġdo Ġyou\n",
      "da   : % O      I-  I-  I- % I-  I-    I-  I-  \n",
      "\n",
      "words: Ġmean Ġ  <TURN> Ġbetween Ġthe Ġn -  Ġhead Ġof Ġthe\n",
      "da   : I-    qw O      I-       I-   I- sd I-    I-  I-  \n",
      "\n",
      "words: Ġnail Ġand Ġthe Ġhandle bar Ġis Ġ <TURN> Ġno Ġ \n",
      "da   : I-    I-   I-   I-      I-  I-  % O      nn  nn\n",
      "\n",
      "words: <TURN> Ġthere Ġany Ġspace Ġ  <TURN> Ġyou Ġcan 't Ġthe\n",
      "da   : O      I-     I-   sd     sd O      I-   I-   I- I-  \n",
      "\n",
      "words: Ġhead Ġof Ġthe Ġnail Ġthe Ġ <TURN> Ġyeah Ġ <TURN>\n",
      "da   : I-    I-  I-   I-    I-   % O      b     b O     \n",
      "\n",
      "words: Ġthe Ġflat Ġpart Ġof Ġthe Ġnail Ġ <TURN> Ġright Ġ\n",
      "da   : I-   I-    I-    I-  I-   I-    % O      b      b\n",
      "\n",
      "words: <TURN> Ġis Ġcovering Ġthe Ġtop Ġpart Ġof Ġthe Ġhandle bar\n",
      "da   : O      I-  I-        I-   I-   I-    I-  I-   I-      I- \n",
      "\n",
      "words: Ġwhere Ġyou Ġwould Ġgrip Ġthe Ġlawn m  ower Ġyou Ġ\n",
      "da   : I-     I-   I-     I-    I-   sd    sd sd   I-   %\n",
      "\n",
      "words: <TURN> Ġokay Ġ <TURN> Ġcan 't Ġsee Ġthat Ġat Ġall\n",
      "da   : O      bk    b O      I-   I- I-   I-    I-  I-  \n",
      "\n",
      "words: Ġ <TURN> Ġat Ġall Ġ <TURN> Ġat Ġall Ġso ĠI\n",
      "da   : % O      I-  I-   % O      I-  sd   sd  I-\n",
      "\n",
      "words: Ġwould Ġsay Ġif Ġyou Ġlook Ġat Ġthe Ġhead Ġof Ġthe\n",
      "da   : I-     I-   I-  I-   I-    I-  I-   I-    I-  I-  \n",
      "\n",
      "words: Ġnail Ġum Ġand Ġkinda Ġdrew Ġa Ġstraight Ġline Ġthrough Ġit\n",
      "da   : I-    I-  I-   I-     I-    I- I-        I-    I-       I- \n",
      "\n",
      "words: Ġlike Ġit Ġwould Ġcut Ġlike Ġit Ġwould Ġit Ġwould Ġline\n",
      "da   : I-    I-  I-     I-   I-    I-  I-     I-  I-     I-   \n",
      "\n",
      "words: Ġup Ġwith Ġthe Ġhandle bar Ġ  <TURN> Ġokay Ġwait Ġwait\n",
      "da   : I-  I-    I-   I-      I-  sd O      b     I-    I-   \n",
      "\n",
      "words: Ġis Ġthe Ġnail Ġlike Ġliterally Ġon Ġtop Ġof Ġthe Ġlawn\n",
      "da   : I-  I-   I-    I-    I-         I-  I-   I-  I-   qy   \n",
      "\n",
      "words: m  ower Ġ  <TURN> Ġliterally Ġ  <TURN> Ġokay Ġokay Ġokay\n",
      "da   : qy qy   qy O      sd         sd O      b     b     b    \n",
      "\n",
      "words: Ġ <TURN> Ġyeah Ġ <TURN> ĠI Ġwas Ġdoing Ġsomething Ġcompletely\n",
      "da   : % O      b     b O      I- I-   I-     I-         I-         \n",
      "\n",
      "words: Ġdifferent Ġ  <TURN> Ġoh Ġyeah Ġit 's Ġlike Ġon Ġtop\n",
      "da   : sd         sd O      I-  ny    I-  I- I-    I-  I-  \n",
      "\n",
      "words: Ġof Ġit Ġ  <TURN> Ġokay Ġso Ġthe Ġbelow Ġthe Ġhead\n",
      "da   : I-  sd  sd O      bk    I-  I-   I-     I-   I-   \n",
      "\n",
      "words: Ġof Ġthe Ġnail Ġthere 's Ġlike Ġa Ġring Ġlike Ġa\n",
      "da   : I-  I-   I-    I-     I- I-    I- I-    I-    I-\n",
      "\n",
      "words: Ġrun g  Ġa Ġbottom Ġline Ġ  <TURN> Ġright ĠI Ġsee\n",
      "da   : I-   I- I- I-      sd    sd O      aa     I- I-  \n",
      "\n",
      "words: Ġthat Ġthat Ġlines Ġup Ġ  <TURN> Ġwith Ġ  <TURN> Ġw\n",
      "da   : I-    I-    I-     I-  sd O      I-    I- O      I-\n",
      "\n",
      "words: - Ġ <TURN> Ġthe Ġmiddle Ġcross bar Ġ  <TURN> Ġcorrect\n",
      "da   : % b O      I-   I-      sd     sd  sd O      aa      \n",
      "\n",
      "words: Ġ  <TURN> </s> Ġokay\n",
      "da   : aa O      b    sd   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "SWDA_DIALOG_ACT_TO_TAG = {\n",
    "    \"Statement-non-opinion\": \"sd\",\n",
    "    \"Acknowledge-Backchannel\": \"b\",\n",
    "    \"Statement-opinion\": \"sv\",\n",
    "    \"Agree-Accept\": \"aa\",\n",
    "    \"Abandoned-or-Turn-Exit\": \"%\",\n",
    "    \"Appreciation\": \"ba\",\n",
    "    \"Yes-No-Question\": \"qy\",\n",
    "    \"Non-verbal\": \"x\",\n",
    "    \"Yes-answers\": \"ny\",\n",
    "    \"Conventional-closing\": \"fc\",\n",
    "    \"Uninterpretable\": \"%\",\n",
    "    \"Wh-Question\": \"qw\",\n",
    "    \"No-answers\": \"nn\",\n",
    "    \"Response-Acknowledgement\": \"bk\",\n",
    "    \"Hedge\": \"h\",\n",
    "    \"Declarative-Yes-No-Question\": \"qy^d\",\n",
    "    # Replace: \"Other\": \"fo_o_fw_by_bc\" with the following as it appears like that in SWDA\n",
    "    \"Other\": 'fo_o_fw_\"_by_bc',\n",
    "    \"Backchannel-in-question-form\": \"bh\",\n",
    "    \"Quotation\": \"^q\",\n",
    "    \"Summarize/reformulate\": \"bf\",\n",
    "    \"Affirmative-non-yes-answers\": \"na\",\n",
    "    \"Action-directive\": \"ad\",\n",
    "    \"Collaborative-Completion\": \"^2\",\n",
    "    \"Repeat-phrase\": \"b^m\",\n",
    "    \"Open-Question\": \"qo\",\n",
    "    \"Rhetorical-Questions\": \"qh\",\n",
    "    \"Hold-before-answer-agreement\": \"^h\",\n",
    "    \"Reject\": \"ar\",\n",
    "    \"Negative-non-no-answers\": \"ng\",\n",
    "    \"Signal-non-understanding\": \"br\",\n",
    "    \"Other-answers\": \"no\",\n",
    "    \"Conventional-opening\": \"fp\",\n",
    "    \"Or-Clause\": \"qrr\",\n",
    "    \"Dispreferred-answers\": \"arp_nd\",\n",
    "    \"3rd-party-talk\": \"t3\",\n",
    "    \"Offers-Options-Commits\": \"oo_co_cc\",\n",
    "    \"Self-talk\": \"t1\",\n",
    "    \"Downplayer\": \"bd\",\n",
    "    \"Maybe-Accept-part\": \"aap_am\",\n",
    "    \"Tag-Question\": \"^g\",\n",
    "    \"Declarative-Wh-Question\": \"qw^d\",\n",
    "    \"Apology\": \"fa\",\n",
    "    \"Thanking\": \"ft\",\n",
    "    \"+\": \"+\"\n",
    "}\n",
    "\n",
    "def predict_da(dfs,n_sess,n_task,id2label):\n",
    "    inp = create_input(dfs,n_sess,n_task)\n",
    "    words = tokenizer.tokenize(inp)\n",
    "    words.insert(0,\"<s>\")\n",
    "    words.insert(-1,\"</s>\")\n",
    "    \n",
    "    encoded_input = tokenizer(inp,return_tensors=\"pt\")\n",
    "    logits = model(**encoded_input).logits\n",
    "    predictions = logits.argmax(dim=-1).cpu().detach().numpy()\n",
    "    predictions_labels = [id2label[i] for i in predictions[0]]\n",
    "\n",
    "    print_aligned(pd.DataFrame({\"words\": words, \"da\": predictions_labels}),width=10)\n",
    "    \n",
    "    \n",
    "words, turns, social_variables = load_data()\n",
    "dfs = merge_turns(words)\n",
    "\n",
    "n_sess, n_task = 2, 4\n",
    "id2label = {int(i): SWDA_DIALOG_ACT_TO_TAG[label] if label in SWDA_DIALOG_ACT_TO_TAG else label for i, label in model.config.id2label.items()}\n",
    "# id2label = {int(i): label for i, label in model.config.id2label.items()}\n",
    "predict_da(dfs,n_sess,n_task,id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay so the mime um is directly above the owl and to the left I mean t- excuse me to the right of the ear so right in the <TURN> and it's parallel with the ear <TURN> exactly\n",
      "\n",
      "[\"okay so the mime um is directly above the owl and to the left I mean t- excuse me to the right of the ear so right in the <TURN> and it's parallel with the ear <TURN> exactly\", '']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:715\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 715\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 46 at dim 1 (got 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(preds)\n\u001b[0;32m---> 28\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_das\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m preds\n",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36mpredict_das\u001b[0;34m(id2label)\u001b[0m\n\u001b[1;32m     10\u001b[0m         inps\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(inps)\n\u001b[0;32m---> 12\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2488\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2487\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2488\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2574\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2569\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2570\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2571\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2572\u001b[0m         )\n\u001b[1;32m   2573\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2595\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2596\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2612\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2613\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2765\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2756\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2757\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2758\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2762\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2763\u001b[0m )\n\u001b[0;32m-> 2765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2767\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/longformer/tokenization_longformer_fast.py:278\u001b[0m, in \u001b[0;36mLongformerTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m )\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:477\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:731\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    727\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    728\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m             )\n\u001b[0;32m--> 731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m         )\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "def predict_das(id2label):\n",
    "    words, turns, social_variables = load_data()\n",
    "    dfs = merge_turns(words)\n",
    "    \n",
    "    inps = []\n",
    "    for n_sess in range(1,2):\n",
    "        for n_task in range(1,3):\n",
    "            inp = create_input(dfs,n_sess,n_task)\n",
    "            print(inp)\n",
    "            inps.append(inp)\n",
    "    print(inps)\n",
    "    encoded_input = tokenizer(inps,return_tensors=\"pt\")\n",
    "    logits = model(**encoded_input).logits\n",
    "    predictions = logits.argmax(dim=-1).cpu().detach().numpy()\n",
    "    preds = []\n",
    "    i = 0\n",
    "    for n_sess in range(1,2):\n",
    "        for n_task in range(1,3):\n",
    "            preds.append({\n",
    "                \"session\": n_sess,\n",
    "                \"task\": n_task,\n",
    "                \"da\": \" \".join([id2label[ii] for ii in predictions[i]]),\n",
    "                \"sv\": social_variables[n_sess][n_task]\n",
    "            })\n",
    "            i += 1\n",
    "    return pd.DataFrame.from_records(preds)\n",
    "\n",
    "preds = predict_das(id2label)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
